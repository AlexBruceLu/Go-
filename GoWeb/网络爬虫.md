
# 爬虫概念：

    访问web服务器，获取指定数据信息的一段程序。工作流程：    
    1. 明确目标 Url   
    2. 发送请求，获取应答数据包。 http.Get(url)    
    3. 过滤 数据。提取有用信息。        
    4. 使用、分析得到数据信息。

# 百度贴吧爬虫实现：

    1. 提示用户指定 起始、终止页。 创建working函数    
    2. 使用 start、end 循环 爬取每一页数据    
    3. 获取 每一页的 URL —— 下一页 = 前一页 + 50   
    4. 封装、实现 HttpGet() 函数，爬取一个网页的数据内容，通过 result 返回。        http.Get/ resp.Body.Close/ buf := make（4096）/
    for {  resp.Body.Read(buf)/   result += string(buf[:n])  return }   
    5. 创建 .html 文件。 使用循环因子 i 命名。    
    6. 将 result 写入 文件 f.WriteString（result）。   
    f.close()  不推荐使用 defer 。

```go
package main

import (
    "fmt"
    "io"
    "net/http"
    "strconv"
    "os"
)

func ReptileTieba(url string) (result string, err error) {
    resp, err1 := http.Get(url)
    if err1 != nil {
        err = err1
    }
    defer resp.Body.Close()
    buf := make([]byte, 4096)
    for {
        n, err2 := resp.Body.Read(buf)
        if n == 0 {
            fmt.Println("该网页爬取完成！")
            break
        }
        if err2 != nil && err2 != io.EOF {
           err = err2
            return
        }
        result += string(buf[:n])
    }
    return
}
func Working(start int, end int) {
    fmt.Printf("正在爬取第%d页，到第%d页\n", start, end)
    for i := start; i <= end; i++ {
        url := "https://tieba.baidu.com/f? kw=%E8%8B%B1%E9%9B%84%E8%81%94%E7%9B%9F&ie=utf-8&pn=" + strconv.Itoa(i )
        result, err := ReptileTieba(url)
        if err != nil {
            fmt.Println("ReptileTieba err:", err)
            continue
        }
        // fmt.Println("result=", result)
        // 将读到的整网页数据，保存成一个文件
        f, err := os.Create("第 " + strconv.Itoa(i) + " 页" + ".html")
        if err != nil {
            fmt.Println("Create err:", err)
            continue
        }
        f.WriteString(result)
        f.Close()
    }
}

func main() {
var start, end int
fmt.Println("请输入爬取的起始页（>=1）：")
fmt.Scan(&start)
fmt.Println("请输入爬取的终止页（>=1）：")
fmt.Scan(&end)
Working(start, end)
}
```

# 并发版百度爬虫：

    1. 封装 爬取一个网页内容的 代码 到  SpiderPage（index）函数中    
    2. 在 working 函数 for 循环启动 go 程 调用 SpiderPage() —— > n个待爬取页面，对应n个go程    
    3. 为防止主 go 程提前结束，引入 channel 实现同步。 SpiderPage（index，channel）    
    4. 在SpiderPage() 结尾处（一个页面爬取完成）， 向channel中写内容 。 channel <- index    
    5.  在 working 函数 添加新 for 循环， 从 channel 不断的读取各个子 go 程写入的数据。 n个子go程 —— 写n次channel —— 读n次channel
   
```go
package main

import (
    "fmt"
    "strconv"
    "os"
    "net/http"
    "time"
    "io"
)

func HttpGet(url string) (result string, err error) {
    resp, err1 := http.Get(url)
    if err1 != nil {
        err = err1
        return
    }
    defer resp.Body.Close()

    time.Sleep(time.Second * 2)
    buf := make([]byte, 4096)
    for {
        n, err2 := resp.Body.Read(buf)
        if n == 0 {
            fmt.Println("文件已读完！")
            return
        }
        if err2 != nil && err2 != io.EOF {
            err = err2
            return
        }
        result += string(buf[:n])
    }
    return
}
func RespTile(i int, page chan int) {
    url := "https://tieba.baidu.com/f?kw=%E8%8B%B1%E9%9B%84%E8%81%94%E7%9B%9F&ie=utf-8&pn=" + strconv.Itoa((i-1)*50)
    result, err := HttpGet(url)
    if err != nil {
        fmt.Println("HttpGet err ", err)
        return
    }
    f, err := os.Create("第" + strconv.Itoa(i) + "页.html")
    if err != nil {
        fmt.Println("os.Create err ", err)
        return
    }
    f.WriteString(result)
    f.Close()
    page <- i
}
func Working1(start, end int) {
    fmt.Printf("正在爬取%d,到%d页...\n", start, end)
    page := make(chan int)
    for i := start; i <= end; i++ {
        go RespTile(i, page)
    }
    for i := start; i <= end; i++ {
        fmt.Printf("第%d页已爬取完成\n ", <-page)
    }
}
func main() {
    // 指定爬取起始、终止页
    var start, end int
    fmt.Println("请输入起始页：")
    fmt.Scan(&start)
    fmt.Println("请输入终止页：")
    fmt.Scan(&end)

    Working1(start, end)
    //封装爬取函数
}
```

#正则表达式：

    能使用 string、strings、strcnov 包函数解决的问题，首选使用库函数。 其次再选择正则表达式。----字符：    
    “.”: 匹配任意一个字符    
    "[ ]": 匹配 [ ] 内任意一个字符。    
    “-”：指定范围： a-z、A-Z、0-9    
    "^": 取反。 使用在 [ ] 内部。[^xy]8   
    [[:digit:]] ——> 数字 == [0-9]-----次数：    
    “?”: 匹配 前面 单元出现 0-1次    
    “+”：匹配 前面 单元 出现 1-N次    
    “*”：匹配 前面 单元 出现 0-N次    
    “{N}”: 匹配 前面 单元  精确匹配 N 次    
    "{N,}": 匹配 前面 单元 至少匹配 N 次    
    "{N,M}": 匹配 前面 单元 匹配 N -- M 次。---- 单元限定符：        
    “()”: 可以将一部分正则表达式，组成一个 单元，可以对该单元使用 数量限定符    

# Go语言使用正则：

    步骤：-----次数：    
    1. 解析编译正则表达式：        MustCompile(str string) *Regexp        参数：正则表达式： 建议使用“反引号”—— ` `        返回值： 编译后的正则表达式 （结构体类型）    
    2. 提取需要的数据：        
    func (re *Regexp) FindAllStringSubmatch(s string, n int) [][]string        参数1：待匹配的字符串。        
    参数2：匹配次数。 -1 表匹配全部        
    返回值： 存储匹配结果的 [ ][ ]string            
    数组的每一个成员都有 string1 和 string2 两个元素。                string1：表示， 带有匹配参考项的字符串。 【0】                                string2：表示，不包含匹配参考项的字符串内容。【1】----- 提取网页标签数据：    举例： 提取 <div></div> 之中的数据        
        1） <div>(.*)</div>    --> 可以提取div标签的内容        
        2） 对于 div 标签中 含有多行内容清空：            
    正则表达式：(?s:(.*?))    
    双向爬取：    
    横向：以页为单位。    纵向：以一个页面内的条目为单位。横向：
    https://movie.douban.com/top250?start=0&filter=        1https://movie.douban.com/top250?start=25&filter=        2https://movie.douban.com/top250?start=50&filter=        3https://movie.douban.com/top250?start=75&filter=       
    4纵向：    
    电影名称： <img width="100" alt="电影名称"            ——> `<img width="100" alt="(.*?)"`    分数：<span class="rating_num" property="v:average">分数</span>    ——> `<span class="rating_num" property="v:average">(.*?)</span>`    评分人数：<span> 评分人数   人评价</span>            ——> `<span>(.*?)人评价</span>`

# 爬取豆瓣电影信息：

    ---- 实现流程：   
    1.  获取用户输入 起始、终止页、启动 toWork 函数 循环 调用 SpiderPageDB(url) 爬取每一个页面    
    2.  SpiderPageDB 中， 获取 豆瓣电影 横向爬取url 信息。封装 HttpGet 函数，爬取一个页面所有数据 存入 result 返回    
    3.  找寻、探索豆瓣网页 纵向爬取规律。找出“电影名”、“分数”、“评分人数”网页数据特征。    
    4. 分别 对这三部分数据使用 go 正则函数：
     1） 解析、编译正则表达式 
     2） 提取信息 ——>  string[1]: 代表没有  匹配参考项内容。   
    5. 将提取到的数据，按自定义格式写入文件。使用 网页编号命名文件。   
    6. 实现并发。     
        1） go SpiderPageDB(url) 。            
        2） 创建 channel 防止主 go 程退出            
        3） SpiderPageDB 函数末尾，写入 channel            
        4） 主 go 程 for  读取 channel 。                               

```go
package main

import (
    "fmt"
    "strconv"
    "net/http"
    "io"
    "regexp"
    "os"
    "time"
)

func Save2file(i int, filmname, filmscore, peopleofnum [][]string) {
    f, err := os.Create("D:/go/go语言高级/day10/src/第" +     strconv.Itoa(i) + "页.txt")
    if err != nil {
        fmt.Println("os.Create err", err)
        return
    }
    defer f.Close()
    //n:=len(filmname)
    fmt.Println("filmname:", len(filmname))
    fmt.Println("2:", len(filmscore))
    fmt.Println("3:", len(peopleofnum))
    f.WriteString("电影名称" + "\t\t\t" + "电影评分" + "\t\t\t" + "评分人数" + "\r\n")
    for i, v := range filmname {
        //f.WriteString(v[1]+"\t\t\t"+filmscore[i][1]+"\t\t\t"+peopleofnum[i][1]+"\r\n")
        f.WriteString(v[1])

        for j := 0; j < (44 - len(v[1])); j++ {
            f.WriteString(" ")
        }
        f.WriteString("\t")
        f.WriteString(filmscore[i][1])
        for j := 0; j < (44 - len(filmscore[i][1])); j++ {
            f.WriteString(" ")
        }
        f.WriteString("\t")
        f.WriteString(peopleofnum[i][1])
        f.WriteString("\r\n")
        //fmt.Println(len(v[1]))
    }
}
func HttpGetDB(url string) (result string, err error) {
    resp, err1 := http.Get(url)
    if err1 != nil {
        fmt.Println("http.Get err ", err1)
        return
    }
    defer resp.Body.Close()
    buf := make([]byte, 4096)
    for {
        n, err2 := resp.Body.Read(buf)
        if n == 0 {
            fmt.Println("文件已读完毕！")
            return
        }
        if err2 != nil && err2 != io.EOF {
            fmt.Println("resp.Body.Read err", err2)
            return
        }
        result += string(buf[:n])
    }
    return
}
func RespTileBD(i int, page chan int) {
    url := "https://movie.douban.com/top250?start=" + strconv.Itoa((i-1)*25) + "&filter="
    result, err := HttpGetDB(url)
    if err != nil {
        fmt.Println("HttpGetDB err ", err)
        return
    }
    ret1 := regexp.MustCompile(`<img width="100" alt="(.*?)" src="`)
    filmname := ret1.FindAllStringSubmatch(result, -1)
    ret2 := regexp.MustCompile(`<span class="rating_num" property="v:average">(.*?)</span>`)
    filmscore := ret2.FindAllStringSubmatch(result, -1)
    ret3 := regexp.MustCompile(`<span>(.*?)人评价</span>`)
    peopleofnum := ret3.FindAllStringSubmatch(result, -1)
    Save2file(i, filmname, filmscore, peopleofnum)
    page <- i
}
func WorkingDB(start, end int) {
    fmt.Printf("正在爬取%d，到%d页...\n", start, end)
    page := make(chan int)
    for i := start; i <= end; i++ {
        //rand.Seed(time.Now().UnixNano())
        //n:=rand.Intn(5)+1
        time.Sleep(2 * time.Second)
        go RespTileBD(i, page)
    }
    for i := start; i <= end; i++ {
        fmt.Printf("%d页爬取完成\n", <-page)
    }
}
func main() {
    var start, end int
    fmt.Println("请输入起始页：")
    fmt.Scan(&start)
    fmt.Println("请输入终止页：")
    fmt.Scan(&end)
    WorkingDB(start, end)
}
```
